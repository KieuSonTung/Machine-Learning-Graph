{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7wEi9LbwdZm"
   },
   "source": [
    "# Link Prediction using DGL\n",
    "\n",
    "This notebook uses the Python Library DGL for friend recommendation. \n",
    "\n",
    "Some of the code is adapted from the following tutorial for link prediction: https://docs.dgl.ai/en/0.6.x/tutorials/blitz/4_link_predict.html#sphx-glr-tutorials-blitz-4-link-predict-py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xgX74djxx08W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a29d8e9f-047a-4831-d649-a1be96878d58"
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import os"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Ru0L4qtsht"
   },
   "source": [
    "## Dataset Loading\n",
    "\n",
    "We use the [Facebook social circles](https://snap.stanford.edu/data/ego-Facebook.html) dataset, available at the Stanford SNAP website, for friend recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QHeHXzNLKSp"
   },
   "source": [
    "\n",
    "Obviously, we need to load the friendship (edges) into the graph. The files also provide user features information, represented by a 224-dimensional vector of 0s and 1s, so we load that as well. For the simplicity of this tutorial, we will only use one ego network (e.g. User 0's network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['0', '107', '1684', '1912', '3437', '348', '3980', '414', '686',\n       '698'], dtype=object)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user(x):\n",
    "    return x[0]\n",
    "\n",
    "user_data = pd.Series(np.char.split(np.array(os.listdir('./facebook_data')), sep='.'))\n",
    "user_ls = np.unique(np.array(user_data.apply(get_user)))\n",
    "user_ls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CURrSLeNPTmV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "28a64487-ee94-49cd-d53d-95e42522e25d"
   },
   "source": [
    "# user id, which correspond to file name prefix\n",
    "USER = 0\n",
    "\n",
    "file_edges = f'facebook_data/{USER}.edges'\n",
    "file_feat = f'facebook_data/{USER}.feat'\n",
    "\n",
    "edges_u, edges_v = [], []\n",
    "\n",
    "with open(file_edges) as f:  # load edges file\n",
    "    for line in f:\n",
    "        e1, e2 = tuple(int(x) - 1 for x in line.split())\n",
    "        edges_u.append(e1)\n",
    "        edges_v.append(e2)\n",
    "\n",
    "edges_u, edges_v = np.array(edges_u), np.array(edges_v)\n",
    "\n",
    "num_nodes = 0  # assumes nodes are sequential\n",
    "feats = []  # node features\n",
    "\n",
    "with open(file_feat) as f:  # load node features file\n",
    "    for line in f:\n",
    "        num_nodes += 1\n",
    "        a = [int(x) for x in line.split()[1:]]\n",
    "        feats.append(torch.tensor(a, dtype=torch.float))\n",
    "\n",
    "feats = torch.stack(feats)\n",
    "# print(feats.shape)\n",
    "\n",
    "g = dgl.graph((edges_u, edges_v))  # construct graph\n",
    "g.ndata['feat'] = feats\n",
    "\n",
    "g"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Graph(num_nodes=347, num_edges=5038,\n      ndata_schemes={'feat': Scheme(shape=(224,), dtype=torch.float32)}\n      edata_schemes={})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWlsfnO8vhuT"
   },
   "source": [
    "## Graph Visualization\n",
    "\n",
    "We use the networkx library for visualizing the friendship relationships. There are hundreds of nodes (not shown), and thousands of links, so it can seem a bit messy when the graph is drawn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "toeZj52TVDeR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "outputId": "116c13b6-8815-44cc-b38b-0c9960257528"
   },
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(set(edges_u + edges_v))\n",
    "G.add_edges_from(zip(edges_u, edges_v))\n",
    "\n",
    "nx.draw(G, node_size=0)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgwhgCwJycku"
   },
   "source": [
    "## Training/testing data construction\n",
    "\n",
    "Once we have the graph constructed, we can split it into the training set and testing set. We omit the validation set for simplicity.\n",
    "\n",
    "We construct a graph of \"negative edges\"; whenever there is a lack of an edges between two nodes `u` and `v` in the original graph, there is a corresponding edge in the negative graph. This is needed for telling the training algorithm what the \"negative examples\" are.\n",
    "\n",
    "We randomly split 30% of links for testing, and leave 70% for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DL6aBJg2xIoc"
   },
   "source": [
    "TEST_RATIO = 0.3\n",
    "\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * TEST_RATIO)\n",
    "train_size = g.number_of_edges() - test_size\n",
    "\n",
    "# get positive edges for test and train\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "# split the negative edges for training and testing \n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "# construct positive and negative graphs for training and testing\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "# training graph\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "train_g = dgl.add_self_loop(train_g)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPYzM9IY5gg4"
   },
   "source": [
    "## Model definition\n",
    "\n",
    "Here we build two models from the existing implementation in the DGL library. Both models consists of 2 convolutional layers. \n",
    "\n",
    "`GCN` is a standard graph convolutional network, and serves as a baseline. `GraphSAGE` is a more advanced and powerful network that we're testing.\n",
    "\n",
    "We also define the `DotPredictor`, which helps to predict the likelihood of whether an edge exists or not, by taking the dot product of two nodes. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UqkZIiV_xLSs"
   },
   "source": [
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "# ----------- create model -------------- #\n",
    "# build an ordinary GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdTqfsc7tfsQ"
   },
   "source": [
    "## Training and testing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nzG_l64dxQ5S"
   },
   "source": [
    "def pipeline(model_name='GCN', hidden_size=16):\n",
    "    # model_name can be GCN or SAGE\n",
    "    # hidden_size is the size of the hidden layer in the neural net\n",
    "    if model_name == 'GCN':\n",
    "        model = GCN(train_g.ndata['feat'].shape[1], hidden_size)\n",
    "    elif model_name == 'SAGE':\n",
    "        model = GraphSAGE(train_g.ndata['feat'].shape[1], hidden_size)\n",
    "        \n",
    "    pred = DotPredictor()\n",
    "\n",
    "    def compute_loss(pos_score, neg_score):  # computes the loss based on binary cross entropy\n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "\n",
    "        print('Scores:', scores)\n",
    "        print('Labels:', labels)\n",
    "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "    def compute_auc(pos_score, neg_score):  # computes AUC (Area-Under-Curve) score\n",
    "        scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "        return roc_auc_score(labels, scores)\n",
    "\n",
    "\n",
    "    # ----------- set up loss and optimizer -------------- #\n",
    "    # in this case, loss will in training loop\n",
    "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "    # ----------- training -------------------------------- #\n",
    "    all_logits = []\n",
    "    for e in range(100):\n",
    "        # forward\n",
    "        h = model(train_g, train_g.ndata['feat'])  # get node embeddings\n",
    "        pos_score = pred(train_pos_g, h)\n",
    "        neg_score = pred(train_neg_g, h)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {}'.format(e, loss))\n",
    "\n",
    "    # ----------- test and check results ---------------- #\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    with torch.no_grad():\n",
    "        pos_score = pred(test_pos_g, h)\n",
    "        neg_score = pred(test_neg_g, h)\n",
    "        print('AUC', compute_auc(pos_score, neg_score))\n",
    "    \n",
    "    return h  # return node embeddings\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnMhM7PTwwXk"
   },
   "source": [
    "### Training and testing GCN:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2a67Jr2_CpI",
    "outputId": "0b0f5f9b-6495-44f3-ce71-9988f9fbee60"
   },
   "source": [
    "h = pipeline(\"GCN\")"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: tensor([0.1035, 0.1435, 0.0843,  ..., 0.0189, 0.0275, 0.0589],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 0, loss: 0.6874449253082275\n",
      "Scores: tensor([0.8192, 0.9109, 0.4737,  ..., 0.1298, 0.1367, 0.3215],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([2.4257, 2.5819, 1.3865,  ..., 0.3597, 0.3136, 0.8315],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([2.2778, 2.3517, 1.0507,  ..., 0.3057, 0.2340, 0.5859],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([1.9235, 1.8989, 0.6141,  ..., 0.2208, 0.1310, 0.2827],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([1.9825, 1.8461, 0.3450,  ..., 0.1894, 0.0518, 0.0868],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 5, loss: 0.5765877366065979\n",
      "Scores: tensor([ 2.5613,  2.2790,  0.2046,  ...,  0.2126, -0.0062, -0.0577],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 3.5164,  3.0559,  0.1515,  ...,  0.2702, -0.0662, -0.2330],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.4375,  3.7829,  0.0615,  ...,  0.3143, -0.1579, -0.5365],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.7893,  3.9621, -0.2011,  ...,  0.2891, -0.2817, -1.0067],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.6772,  3.7503, -0.4927,  ...,  0.2232, -0.4038, -1.4891],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 10, loss: 0.495160311460495\n",
      "Scores: tensor([ 4.6225,  3.6421, -0.6847,  ...,  0.1811, -0.4969, -1.8572],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.9650,  3.9188, -0.8026,  ...,  0.1944, -0.5662, -2.0952],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4209,  4.3293, -0.8483,  ...,  0.2348, -0.6047, -2.2046],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.5834,  4.5059, -0.8489,  ...,  0.2499, -0.6098, -2.1937],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3268,  4.3228, -0.8342,  ...,  0.2329, -0.5832, -2.1088],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 15, loss: 0.48069313168525696\n",
      "Scores: tensor([ 4.8367,  3.9218, -0.7807,  ...,  0.2016, -0.5378, -1.9784],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.4222,  3.5763, -0.6944,  ...,  0.1754, -0.4935, -1.8590],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.2655,  3.4443, -0.6146,  ...,  0.1666, -0.4628, -1.7819],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.3804,  3.5387, -0.5564,  ...,  0.1749, -0.4494, -1.7556],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.6535,  3.7651, -0.5068,  ...,  0.1918, -0.4482, -1.7701],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 20, loss: 0.4735040068626404\n",
      "Scores: tensor([ 4.9070,  3.9680, -0.4491,  ...,  0.2035, -0.4547, -1.8116],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.9977,  4.0231, -0.3699,  ...,  0.1986, -0.4656, -1.8672],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.9049,  3.9157, -0.2776,  ...,  0.1773, -0.4785, -1.9290],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.7372,  3.7428, -0.1924,  ...,  0.1499, -0.4922, -1.9915],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.6444,  3.6281, -0.1530,  ...,  0.1289, -0.5087, -2.0517],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 25, loss: 0.46371251344680786\n",
      "Scores: tensor([ 4.7086,  3.6456, -0.1636,  ...,  0.1213, -0.5295, -2.1065],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.9064,  3.7819, -0.1802,  ...,  0.1262, -0.5526, -2.1589],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1157,  3.9458, -0.1485,  ...,  0.1360, -0.5745, -2.2023],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2004,  4.0186, -0.0675,  ...,  0.1408, -0.5908, -2.2142],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1255,  3.9634,  0.0459,  ...,  0.1368, -0.5968, -2.1781],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 30, loss: 0.4566281735897064\n",
      "Scores: tensor([ 4.9834,  3.8622,  0.1700,  ...,  0.1300, -0.5928, -2.1333],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.8943,  3.8026,  0.2658,  ...,  0.1257, -0.5837, -2.1424],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.8986,  3.8069,  0.2726,  ...,  0.1254, -0.5702, -2.1957],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 4.9653,  3.8457,  0.1810,  ...,  0.1274, -0.5559, -2.2515],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0344,  3.8765,  0.0532,  ...,  0.1285, -0.5426, -2.2918],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 35, loss: 0.4519338309764862\n",
      "Scores: tensor([ 5.0637,  3.8803, -0.0252,  ...,  0.1270, -0.5331, -2.3291],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0506e+00,  3.8653e+00,  3.1700e-03,  ...,  1.2235e-01,\n",
      "        -5.2942e-01, -2.3864e+00], grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0213,  3.8502,  0.1080,  ...,  0.1157, -0.5302, -2.4725],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0093,  3.8466,  0.2022,  ...,  0.1088, -0.5327, -2.5770],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0216,  3.8429,  0.2007,  ...,  0.1019, -0.5344, -2.6761],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 40, loss: 0.44766080379486084\n",
      "Scores: tensor([ 5.0624,  3.8412,  0.1022,  ...,  0.0955, -0.5339, -2.7665],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1199,  3.8495, -0.0188,  ...,  0.0909, -0.5329, -2.8501],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1600,  3.8607, -0.0704,  ...,  0.0868, -0.5332, -2.9248],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1577,  3.8629, -0.0151,  ...,  0.0822, -0.5354, -2.9860],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1238,  3.8591,  0.1137,  ...,  0.0775, -0.5385, -3.0294],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 45, loss: 0.4436977803707123\n",
      "Scores: tensor([ 5.0925,  3.8600,  0.2392,  ...,  0.0745, -0.5413, -3.0522],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.0903,  3.8692,  0.2955,  ...,  0.0738, -0.5436, -3.0538],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1209,  3.8887,  0.2808,  ...,  0.0752, -0.5432, -3.0383],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1652,  3.9158,  0.2479,  ...,  0.0765, -0.5406, -3.0165],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1864,  3.9326,  0.2488,  ...,  0.0766, -0.5371, -2.9888],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 50, loss: 0.4396634101867676\n",
      "Scores: tensor([ 5.1666,  3.9298,  0.2981,  ...,  0.0742, -0.5340, -2.9506],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1248,  3.9162,  0.3656,  ...,  0.0707, -0.5313, -2.8965],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1040,  3.9120,  0.4037,  ...,  0.0687, -0.5292, -2.8315],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.1362,  3.9365,  0.3944,  ...,  0.0691, -0.5275, -2.7725],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2091,  3.9833,  0.3562,  ...,  0.0702, -0.5258, -2.7407],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 55, loss: 0.43557095527648926\n",
      "Scores: tensor([ 5.2627,  4.0157,  0.3214,  ...,  0.0689, -0.5232, -2.7240],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2509,  4.0037,  0.3143,  ...,  0.0638, -0.5196, -2.7006],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2099,  3.9777,  0.3365,  ...,  0.0574, -0.5160, -2.6782],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2068,  3.9850,  0.3638,  ...,  0.0528, -0.5132, -2.6757],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2520,  4.0237,  0.3647,  ...,  0.0511, -0.5102, -2.6748],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 60, loss: 0.431903213262558\n",
      "Scores: tensor([ 5.3064,  4.0629,  0.3442,  ...,  0.0506, -0.5057, -2.6584],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3278,  4.0786,  0.3349,  ...,  0.0491, -0.4993, -2.6315],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3114,  4.0764,  0.3595,  ...,  0.0461, -0.4913, -2.6114],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2760,  4.0651,  0.4019,  ...,  0.0434, -0.4826, -2.5853],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2528,  4.0570,  0.4291,  ...,  0.0425, -0.4738, -2.5382],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 65, loss: 0.42859938740730286\n",
      "Scores: tensor([ 5.2731,  4.0752,  0.4360,  ...,  0.0442, -0.4654, -2.4887],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3197,  4.1144,  0.4438,  ...,  0.0471, -0.4576, -2.4518],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3336,  4.1306,  0.4652,  ...,  0.0485, -0.4501, -2.4103],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3040,  4.1128,  0.4962,  ...,  0.0479, -0.4428, -2.3599],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.2819,  4.0994,  0.5245,  ...,  0.0470, -0.4363, -2.3187],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 70, loss: 0.42549046874046326\n",
      "Scores: tensor([ 5.3090,  4.1204,  0.5356,  ...,  0.0479, -0.4310, -2.2974],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3561,  4.1498,  0.5318,  ...,  0.0498, -0.4265, -2.2714],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3842,  4.1694,  0.5427,  ...,  0.0514, -0.4226, -2.2458],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3886,  4.1855,  0.5766,  ...,  0.0525, -0.4192, -2.2414],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.3803,  4.1904,  0.5997,  ...,  0.0531, -0.4145, -2.2382],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 75, loss: 0.4224821627140045\n",
      "Scores: tensor([ 5.3826,  4.1897,  0.5898,  ...,  0.0535, -0.4081, -2.2221],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4137,  4.2145,  0.5758,  ...,  0.0556, -0.4020, -2.2161],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4404,  4.2474,  0.5852,  ...,  0.0586, -0.3967, -2.2149],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4344,  4.2590,  0.6147,  ...,  0.0606, -0.3904, -2.1992],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4280,  4.2742,  0.6436,  ...,  0.0632, -0.3837, -2.1904],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 80, loss: 0.4194128215312958\n",
      "Scores: tensor([ 5.4326,  4.2883,  0.6476,  ...,  0.0659, -0.3756, -2.1755],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4457,  4.3049,  0.6457,  ...,  0.0692, -0.3672, -2.1566],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4538,  4.3288,  0.6705,  ...,  0.0738, -0.3605, -2.1433],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4473,  4.3472,  0.7175,  ...,  0.0790, -0.3548, -2.1281],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4417,  4.3603,  0.7557,  ...,  0.0844, -0.3489, -2.1113],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 85, loss: 0.4163009822368622\n",
      "Scores: tensor([ 5.4469,  4.3720,  0.7687,  ...,  0.0901, -0.3424, -2.0952],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4491,  4.3800,  0.7808,  ...,  0.0961, -0.3361, -2.0782],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4396,  4.3887,  0.8177,  ...,  0.1031, -0.3312, -2.0637],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4312,  4.4078,  0.8686,  ...,  0.1120, -0.3288, -2.0607],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4322,  4.4181,  0.8930,  ...,  0.1200, -0.3257, -2.0507],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 90, loss: 0.41310393810272217\n",
      "Scores: tensor([ 5.4438,  4.4300,  0.8969,  ...,  0.1284, -0.3234, -2.0468],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4465,  4.4379,  0.9068,  ...,  0.1369, -0.3220, -2.0450],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4382,  4.4415,  0.9342,  ...,  0.1454, -0.3221, -2.0399],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4436,  4.4645,  0.9686,  ...,  0.1560, -0.3253, -2.0475],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4587,  4.4787,  0.9820,  ...,  0.1657, -0.3274, -2.0444],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "In epoch 95, loss: 0.40984007716178894\n",
      "Scores: tensor([ 5.4681,  4.4996,  0.9985,  ...,  0.1775, -0.3313, -2.0567],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4573,  4.5014,  1.0213,  ...,  0.1884, -0.3325, -2.0581],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4517,  4.5160,  1.0527,  ...,  0.2013, -0.3352, -2.0672],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Scores: tensor([ 5.4629,  4.5390,  1.0756,  ...,  0.2147, -0.3385, -2.0752],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Labels: tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "AUC 0.9243320875240516\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV1RZcEow7D8"
   },
   "source": [
    "### Training and testing SAGE:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i03FVEP5_adS",
    "outputId": "8611b3a0-0464-4cb5-de87-67d70c3dbf31"
   },
   "source": [
    "h = pipeline(\"SAGE\")"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.185091495513916\n",
      "In epoch 5, loss: 0.6183627843856812\n",
      "In epoch 10, loss: 0.5766068696975708\n",
      "In epoch 15, loss: 0.5462220311164856\n",
      "In epoch 20, loss: 0.5231993794441223\n",
      "In epoch 25, loss: 0.5026652216911316\n",
      "In epoch 30, loss: 0.489641398191452\n",
      "In epoch 35, loss: 0.47622647881507874\n",
      "In epoch 40, loss: 0.46585485339164734\n",
      "In epoch 45, loss: 0.45505568385124207\n",
      "In epoch 50, loss: 0.44496604800224304\n",
      "In epoch 55, loss: 0.43537989258766174\n",
      "In epoch 60, loss: 0.42571908235549927\n",
      "In epoch 65, loss: 0.415399432182312\n",
      "In epoch 70, loss: 0.40556761622428894\n",
      "In epoch 75, loss: 0.39651331305503845\n",
      "In epoch 80, loss: 0.3878096640110016\n",
      "In epoch 85, loss: 0.37894803285598755\n",
      "In epoch 90, loss: 0.37082192301750183\n",
      "In epoch 95, loss: 0.36288997530937195\n",
      "AUC 0.9054745674889767\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVmQFofwlu3e"
   },
   "source": [
    "# Friend recommendation\n",
    "\n",
    "Once we trained our model, we can finally suggest friends. To do this, we calculate the embedding dot products between a given user and all other users that he/she is currently NOT friends with. Then we choose 5 users with the highest dot product scores to recommend to him/her."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3yTYJKHtfvsh"
   },
   "source": [
    "# --------- generate recommendation for user -------- #\n",
    "def generate_rec(h, user_id=0, num_pred=5):\n",
    "    # `h` represents the node embeddings, with shape [num_nodes, hidden_size]\n",
    "\n",
    "    # generate a graph with (num_nodes - num_friends_of_user) edges\n",
    "    # one end of the edge is user_id\n",
    "    # the other end is a user that's NOT friends with user_id\n",
    "    user_friends = set()\n",
    "    user_neg_u, user_neg_v = [], []\n",
    "    for n1, n2 in zip(u, v):   # get all friends of user_id\n",
    "        if int(n1) == user_id:\n",
    "            user_friends.add(int(n2))\n",
    "        if int(n2) == user_id:\n",
    "            user_friends.add(int(n1))\n",
    "\n",
    "    for i in range(num_nodes):  # generate \"negative edges\" for user_id\n",
    "        if i != user_id and i not in user_friends:\n",
    "            user_neg_u.append(user_id)\n",
    "            user_neg_v.append(i)\n",
    "            \n",
    "    user_g = dgl.graph((user_neg_u, user_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "    pred = DotPredictor()\n",
    "\n",
    "    # calculate the score of each user\n",
    "    scores = [(i, score) for i, score in enumerate(pred(user_g, h))]\n",
    "\n",
    "    # produce final ranked list\n",
    "    scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "    # display results\n",
    "    print(f\"List of 5 suggested friends for user {user_id}:\")\n",
    "    for i in range(num_pred):\n",
    "        print(f'- User {scores[i][0]}, score = {scores[i][1]}')\n",
    "\n",
    "    # data_pred = {scores[i][0]: float(scores[i][1]) for i in range(num_pred)}\n",
    "    data_pred = {'User': [scores[i][0] for i in range(num_pred)],\n",
    "                 'Score': [float(scores[i][1]) for i in range(num_pred)]}\n",
    "    df_pred = pd.DataFrame(data=data_pred, columns=['User', 'Score'])\n",
    "\n",
    "    return df_pred\n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZcZlgbria0d",
    "outputId": "6fd06b4f-d8a9-4d8e-fd99-04ec4b70518a"
   },
   "source": [
    "df_pred = generate_rec(h, user_id=18)\n",
    "df_pred"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 5 suggested friends for user 18:\n",
      "- User 205, score = 7.252294540405273\n",
      "- User 132, score = 6.9791364669799805\n",
      "- User 141, score = 5.800884246826172\n",
      "- User 310, score = 5.4453325271606445\n",
      "- User 231, score = 5.1912007331848145\n"
     ]
    },
    {
     "data": {
      "text/plain": "   User     Score\n0   205  7.252295\n1   132  6.979136\n2   141  5.800884\n3   310  5.445333\n4   231  5.191201",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>205</td>\n      <td>7.252295</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>132</td>\n      <td>6.979136</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>141</td>\n      <td>5.800884</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>310</td>\n      <td>5.445333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>231</td>\n      <td>5.191201</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
